{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from copy import deepcopy\n",
    "tagger = Mecab()\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open(\"contextual.iob\",\"r\",encoding=\"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "history=[[\"<null>\"]]\n",
    "for d in data:\n",
    "    if d==\"\\n\":\n",
    "        history=[[\"<null>\"]]\n",
    "        continue\n",
    "    dd = d.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "    if dd[1]==\"\":\n",
    "        bot = tagger.morphs(dd[0])\n",
    "        history.append(bot)\n",
    "    else:\n",
    "        user = dd[0].split()\n",
    "        tag = dd[1].split()\n",
    "        intent = dd[2]\n",
    "        temp = deepcopy(history)\n",
    "        train_data.append([temp,user,tag,intent])\n",
    "        history.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "historys, currents, slots, intents = list(zip(*train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(currents)))\n",
    "slot_vocab = list(set(flatten(slots)))\n",
    "intent_vocab = list(set(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={\"<pad>\" : 0, \"<unk>\" : 1, \"<null>\" : 2, \"<s>\" : 3, \"</s>\" : 4}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo)==None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "slot2index={\"<pad>\" : 0}\n",
    "for vo in slot_vocab:\n",
    "    if slot2index.get(vo)==None:\n",
    "        slot2index[vo] = len(slot2index)\n",
    "        \n",
    "intent2index={}\n",
    "for vo in intent_vocab:\n",
    "    if intent2index.get(vo)==None:\n",
    "        intent2index[vo] = len(intent2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in train_data:\n",
    "    for i,history in enumerate(t[0]):\n",
    "        t[0][i] = prepare_sequence(history, word2index).view(1, -1)\n",
    "\n",
    "    t[1] = prepare_sequence(t[1], word2index).view(1, -1)\n",
    "    t[2] = prepare_sequence(t[2], slot2index).view(1, -1)\n",
    "    t[3] = Variable(torch.LongTensor([intent2index[t[3]]])).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SDEN(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_size,slot_size,intent_size):\n",
    "        super(SDEN,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        self.bigru_m = nn.GRU(embed_size,hidden_size,batch_first=True,bidirectional=True)\n",
    "        self.bigru_c = nn.GRU(embed_size,hidden_size,batch_first=True,bidirectional=True)\n",
    "        self.context_encoder = nn.Sequential(nn.Linear(hidden_size*4,hidden_size*2),\n",
    "                                                               nn.Sigmoid())\n",
    "        self.session_encoder = nn.GRU(hidden_size*2,hidden_size*2,batch_first=True,bidirectional=True)\n",
    "        \n",
    "        self.decoder_1 = nn.GRU(embed_size,hidden_size*2,batch_first=True,bidirectional=True)\n",
    "        self.decoder_2 = nn.LSTM(hidden_size*4,hidden_size*2,batch_first=True,bidirectional=True)\n",
    "        \n",
    "        self.intent_linear = nn.Linear(hidden_size*4,intent_size)\n",
    "        self.slot_linear = nn.Linear(hidden_size*4,slot_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self,history,current):\n",
    "        batch_size = len(history)\n",
    "        H= [] # encoded history\n",
    "        for h in history:\n",
    "            mask = h.eq(0)\n",
    "            embeds = self.embed(h)\n",
    "            embeds = self.dropout(embeds)\n",
    "            outputs, hidden = self.bigru_m(embeds)\n",
    "            real_hidden = []\n",
    "\n",
    "            for i, o in enumerate(outputs): # B,T,D\n",
    "                real_length = mask[i].data.tolist().count(0) \n",
    "                real_hidden.append(o[real_length - 1])\n",
    "\n",
    "            H.append(torch.cat(real_hidden).view(h.size(0), -1).unsqueeze(0))\n",
    "        \n",
    "        M = torch.cat(H) # B,T_C,2H\n",
    "        M = self.dropout(M)\n",
    "        embeds = self.embed(current)\n",
    "        embeds = self.dropout(embeds)\n",
    "        mask = current.eq(0)\n",
    "        outputs, hidden = self.bigru_c(embeds)\n",
    "        real_hidden=[]\n",
    "        for i, o in enumerate(outputs): # B,T,D\n",
    "            real_length = mask[i].data.tolist().count(0) \n",
    "            real_hidden.append(o[real_length - 1])\n",
    "        C = torch.cat(real_hidden).view(current.size(0),1, -1) # B,1,2H\n",
    "        C = self.dropout(C)\n",
    "        \n",
    "        CONCAT = []\n",
    "        for i in range(batch_size):\n",
    "            m = M[i] # T_c,2H\n",
    "            c = C[i] # 1,2H\n",
    "            c = c.expand_as(m)\n",
    "            cat = torch.cat([m,c],1)\n",
    "            CONCAT.append(cat.unsqueeze(0))\n",
    "        CONCAT = torch.cat(CONCAT)\n",
    "        \n",
    "        G = self.context_encoder(CONCAT)\n",
    "        \n",
    "        _,H = self.session_encoder(G) # 2,B,2H\n",
    "        cell_state = torch.zeros_like(H)\n",
    "        if USE_CUDA: cell_state = cell_state.cuda()\n",
    "        O_1,_ = self.decoder_1(embeds)\n",
    "        O_1 = self.dropout(O_1)\n",
    "        \n",
    "        O_2,(S_2,_) = self.decoder_2(O_1,(H,cell_state))\n",
    "        O_2 = self.dropout(O_2)\n",
    "        S = torch.cat([s for s in S_2],1)\n",
    "        \n",
    "        intent_prob = self.intent_linear(S)\n",
    "        slot_prob = self.slot_linear(O_2.contiguous().view(O_2.size(0)*O_2.size(1),-1))\n",
    "        \n",
    "        return slot_prob, intent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "BATCH = 32\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SDEN(len(word2index),100,64,len(slot2index),len(intent2index))\n",
    "slot_loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "intent_loss_function = nn.CrossEntropyLoss()\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1,milestones=[EPOCH//4,EPOCH//2],optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/2422] mean_loss : 0.589\n",
      "[0/10] [200/2422] mean_loss : 0.363\n",
      "[0/10] [400/2422] mean_loss : 0.357\n",
      "[0/10] [600/2422] mean_loss : 0.342\n",
      "[0/10] [800/2422] mean_loss : 0.345\n",
      "[0/10] [1000/2422] mean_loss : 0.345\n",
      "[0/10] [1200/2422] mean_loss : 0.343\n",
      "[0/10] [1400/2422] mean_loss : 0.348\n",
      "[0/10] [1600/2422] mean_loss : 0.355\n",
      "[0/10] [1800/2422] mean_loss : 0.337\n",
      "[0/10] [2000/2422] mean_loss : 0.339\n",
      "[0/10] [2200/2422] mean_loss : 0.352\n",
      "[0/10] [2400/2422] mean_loss : 0.348\n",
      "[1/10] [0/2422] mean_loss : 0.522\n",
      "[1/10] [200/2422] mean_loss : 0.350\n",
      "[1/10] [400/2422] mean_loss : 0.323\n",
      "[1/10] [600/2422] mean_loss : 0.338\n",
      "[1/10] [800/2422] mean_loss : 0.332\n",
      "[1/10] [1000/2422] mean_loss : 0.343\n",
      "[1/10] [1200/2422] mean_loss : 0.336\n",
      "[1/10] [1400/2422] mean_loss : 0.341\n",
      "[1/10] [1600/2422] mean_loss : 0.330\n",
      "[1/10] [1800/2422] mean_loss : 0.355\n",
      "[1/10] [2000/2422] mean_loss : 0.318\n",
      "[1/10] [2200/2422] mean_loss : 0.335\n",
      "[1/10] [2400/2422] mean_loss : 0.342\n",
      "[2/10] [0/2422] mean_loss : 0.233\n",
      "[2/10] [200/2422] mean_loss : 0.338\n",
      "[2/10] [400/2422] mean_loss : 0.325\n",
      "[2/10] [600/2422] mean_loss : 0.330\n",
      "[2/10] [800/2422] mean_loss : 0.337\n",
      "[2/10] [1000/2422] mean_loss : 0.343\n",
      "[2/10] [1200/2422] mean_loss : 0.328\n",
      "[2/10] [1400/2422] mean_loss : 0.333\n",
      "[2/10] [1600/2422] mean_loss : 0.328\n",
      "[2/10] [1800/2422] mean_loss : 0.335\n",
      "[2/10] [2000/2422] mean_loss : 0.323\n",
      "[2/10] [2200/2422] mean_loss : 0.329\n",
      "[2/10] [2400/2422] mean_loss : 0.337\n",
      "[3/10] [0/2422] mean_loss : 0.513\n",
      "[3/10] [200/2422] mean_loss : 0.321\n",
      "[3/10] [400/2422] mean_loss : 0.316\n",
      "[3/10] [600/2422] mean_loss : 0.334\n",
      "[3/10] [800/2422] mean_loss : 0.334\n",
      "[3/10] [1000/2422] mean_loss : 0.336\n",
      "[3/10] [1200/2422] mean_loss : 0.319\n",
      "[3/10] [1400/2422] mean_loss : 0.311\n",
      "[3/10] [1600/2422] mean_loss : 0.325\n",
      "[3/10] [1800/2422] mean_loss : 0.317\n",
      "[3/10] [2000/2422] mean_loss : 0.331\n",
      "[3/10] [2200/2422] mean_loss : 0.322\n",
      "[3/10] [2400/2422] mean_loss : 0.333\n",
      "[4/10] [0/2422] mean_loss : 0.275\n",
      "[4/10] [200/2422] mean_loss : 0.330\n",
      "[4/10] [400/2422] mean_loss : 0.320\n",
      "[4/10] [600/2422] mean_loss : 0.315\n",
      "[4/10] [800/2422] mean_loss : 0.339\n",
      "[4/10] [1000/2422] mean_loss : 0.343\n",
      "[4/10] [1200/2422] mean_loss : 0.313\n",
      "[4/10] [1400/2422] mean_loss : 0.317\n",
      "[4/10] [1600/2422] mean_loss : 0.321\n",
      "[4/10] [1800/2422] mean_loss : 0.322\n",
      "[4/10] [2000/2422] mean_loss : 0.310\n",
      "[4/10] [2200/2422] mean_loss : 0.318\n",
      "[4/10] [2400/2422] mean_loss : 0.309\n",
      "[5/10] [0/2422] mean_loss : 0.204\n",
      "[5/10] [200/2422] mean_loss : 0.338\n",
      "[5/10] [400/2422] mean_loss : 0.326\n",
      "[5/10] [600/2422] mean_loss : 0.322\n",
      "[5/10] [800/2422] mean_loss : 0.329\n",
      "[5/10] [1000/2422] mean_loss : 0.323\n",
      "[5/10] [1200/2422] mean_loss : 0.325\n",
      "[5/10] [1400/2422] mean_loss : 0.324\n",
      "[5/10] [1600/2422] mean_loss : 0.321\n",
      "[5/10] [1800/2422] mean_loss : 0.326\n",
      "[5/10] [2000/2422] mean_loss : 0.320\n",
      "[5/10] [2200/2422] mean_loss : 0.324\n",
      "[5/10] [2400/2422] mean_loss : 0.327\n",
      "[6/10] [0/2422] mean_loss : 0.389\n",
      "[6/10] [200/2422] mean_loss : 0.325\n",
      "[6/10] [400/2422] mean_loss : 0.332\n",
      "[6/10] [600/2422] mean_loss : 0.325\n",
      "[6/10] [800/2422] mean_loss : 0.317\n",
      "[6/10] [1000/2422] mean_loss : 0.306\n",
      "[6/10] [1200/2422] mean_loss : 0.336\n",
      "[6/10] [1400/2422] mean_loss : 0.307\n",
      "[6/10] [1600/2422] mean_loss : 0.328\n",
      "[6/10] [1800/2422] mean_loss : 0.313\n",
      "[6/10] [2000/2422] mean_loss : 0.323\n",
      "[6/10] [2200/2422] mean_loss : 0.325\n",
      "[6/10] [2400/2422] mean_loss : 0.307\n",
      "[7/10] [0/2422] mean_loss : 0.181\n",
      "[7/10] [200/2422] mean_loss : 0.315\n",
      "[7/10] [400/2422] mean_loss : 0.324\n",
      "[7/10] [600/2422] mean_loss : 0.328\n",
      "[7/10] [800/2422] mean_loss : 0.321\n",
      "[7/10] [1000/2422] mean_loss : 0.320\n",
      "[7/10] [1200/2422] mean_loss : 0.324\n",
      "[7/10] [1400/2422] mean_loss : 0.314\n",
      "[7/10] [1600/2422] mean_loss : 0.310\n",
      "[7/10] [1800/2422] mean_loss : 0.308\n",
      "[7/10] [2000/2422] mean_loss : 0.338\n",
      "[7/10] [2200/2422] mean_loss : 0.319\n",
      "[7/10] [2400/2422] mean_loss : 0.327\n",
      "[8/10] [0/2422] mean_loss : 0.484\n",
      "[8/10] [200/2422] mean_loss : 0.317\n",
      "[8/10] [400/2422] mean_loss : 0.332\n",
      "[8/10] [600/2422] mean_loss : 0.322\n",
      "[8/10] [800/2422] mean_loss : 0.318\n",
      "[8/10] [1000/2422] mean_loss : 0.308\n",
      "[8/10] [1200/2422] mean_loss : 0.315\n",
      "[8/10] [1400/2422] mean_loss : 0.309\n",
      "[8/10] [1600/2422] mean_loss : 0.327\n",
      "[8/10] [1800/2422] mean_loss : 0.324\n",
      "[8/10] [2000/2422] mean_loss : 0.326\n",
      "[8/10] [2200/2422] mean_loss : 0.338\n",
      "[8/10] [2400/2422] mean_loss : 0.311\n",
      "[9/10] [0/2422] mean_loss : 0.179\n",
      "[9/10] [200/2422] mean_loss : 0.326\n",
      "[9/10] [400/2422] mean_loss : 0.308\n",
      "[9/10] [600/2422] mean_loss : 0.320\n",
      "[9/10] [800/2422] mean_loss : 0.322\n",
      "[9/10] [1000/2422] mean_loss : 0.332\n",
      "[9/10] [1200/2422] mean_loss : 0.311\n",
      "[9/10] [1400/2422] mean_loss : 0.333\n",
      "[9/10] [1600/2422] mean_loss : 0.320\n",
      "[9/10] [1800/2422] mean_loss : 0.310\n",
      "[9/10] [2000/2422] mean_loss : 0.323\n",
      "[9/10] [2200/2422] mean_loss : 0.339\n",
      "[9/10] [2400/2422] mean_loss : 0.307\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i,batch in enumerate(data_loader(train_data,BATCH,True)):\n",
    "        h,c,slot,intent = pad_to_batch(batch,word2index,slot2index)\n",
    "        if USE_CUDA:\n",
    "            h = [hh.cuda() for hh in h]\n",
    "            c = c.cuda()\n",
    "            slot = slot.cuda()\n",
    "            intent = intent.cuda()\n",
    "        model.zero_grad()\n",
    "        slot_p, intent_p = model(h,c)\n",
    "\n",
    "        loss_s = slot_loss_function(slot_p,slot.view(-1))\n",
    "        loss_i = intent_loss_function(intent_p,intent.view(-1))\n",
    "        loss = loss_s + loss_i\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(\"[%d/%d] [%d/%d] mean_loss : %.3f\" % (epoch,EPOCH,i,len(train_data)//BATCH,np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "torch.save(model.state_dict(),'sden.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(word2index,open('vocab.pkl','wb'))\n",
    "pickle.dump(slot2index,open('slot.pkl','wb'))\n",
    "pickle.dump(intent2index,open('intent.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
