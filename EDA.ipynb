{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "import re\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 'TO')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_iob(phrase,goal_tracker):\n",
    "    resolution = re.compile('-\\d')\n",
    "    phrase = deepcopy(phrase.lower())\n",
    "    goal_tracker = deepcopy(goal_tracker)\n",
    "    order = [[(k,v),phrase.index(v)] for k,v in goal_tracker.items() if v !=None and v in phrase]\n",
    "    order = sorted(order, key=lambda x: x[1])\n",
    "    goal_tracker = OrderedDict([o[0] for o in order])\n",
    "    pos = nltk.word_tokenize(phrase)\n",
    "    tag=[\"O\"]*len(pos)\n",
    "    for k,v in goal_tracker.items():\n",
    "        B=True\n",
    "        if (v in phrase):\n",
    "            for i,p in enumerate(pos):\n",
    "                if p!=\"<MASK>\" and (p in v):\n",
    "                    if len(p)<=2 and i < len(pos)-1:\n",
    "                        if len(p)==2 and 'N' not in nltk.pos_tag([p])[0][1]:\n",
    "                            continue\n",
    "                        elif pos[i+1] not in v:\n",
    "                            continue\n",
    "                    if B:\n",
    "                        tag[i] = 'B-'+k\n",
    "                        B=False\n",
    "                        pos[i]=\"<MASK>\"\n",
    "                    else:\n",
    "                        if i!=0 and tag[i-1][0] in ['B','I']: \n",
    "                            tag[i] = 'I-'+k\n",
    "                            pos[i]=\"<MASK>\"\n",
    "    tag = [resolution.sub('',t) for t in tag]\n",
    "    return tag\n",
    "\n",
    "\n",
    "def get_phrase(phrase,value):\n",
    "    token = nltk.word_tokenize(phrase.lower())\n",
    "    value = value.lower()\n",
    "    token = [w[0] for w in nltk.pos_tag(token) if w[1].isalpha()]\n",
    "    unigram = [[t] for t in token]\n",
    "    bigram = list(nltk.ngrams(token,2))\n",
    "    trigram = list(nltk.ngrams(token,3))\n",
    "    fourgram = list(nltk.ngrams(token,4))\n",
    "    candit = unigram+bigram+trigram+fourgram\n",
    "    value = nltk.word_tokenize(value)\n",
    "    \n",
    "    result = []\n",
    "    for c in candit:\n",
    "        r = SequenceMatcher(None,c,value)\n",
    "        result.append([r.ratio(),\" \".join(c)])\n",
    "    \n",
    "    result = sorted(result,key=lambda x:x[0],reverse=True)\n",
    "    return result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'B-per']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_iob('he is Good',{'per-1':'he','per-2':'good'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../dataset/kvret/kvret_train_public.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = json.load(open('../dataset/kvret/kvret_entities.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['location', 'weekly_time', 'time', 'weather_attribute', 'agenda', 'poi_type', 'distance', 'traffic_info', 'room', 'event', 'poi', 'party', 'date', 'temperature'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scenario', 'dialogue'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver  :  where's the nearest parking garage\n",
      "assistant  :  The nearest parking garage is Dish Parking at 550 Alester Ave. Would you like directions there? \n",
      "driver  :  Yes, please set directions via a route that avoids all heavy traffic if possible. \n",
      "assistant  :  It looks like there is a road block being reported on the route but I will still find the quickest route to 550 Alester Ave. \n",
      "driver  :  Thanks so much for your help. \n",
      "assistant  :  You're very welcome!\n"
     ]
    }
   ],
   "source": [
    "for d in data[0]['dialogue']:\n",
    "    print(d['turn'],' : ',d['data']['utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../dataset/kvret/kvret_train_public.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags={'navigate' : [], 'schedule' : [], 'weather' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:08<00:00, 277.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for dindex in tqdm(range(len(data))):\n",
    "    diag=[]\n",
    "    length = len(data[dindex]['dialogue'])\n",
    "    meta_task=\"\"\n",
    "    if length<=1: continue\n",
    "    for i in range(length-1):\n",
    "        if data[dindex]['dialogue'][i]['turn']=='driver':\n",
    "            task = data[dindex]['scenario']['task']['intent']\n",
    "            meta_task = task\n",
    "            phrase = data[dindex]['dialogue'][i]['data']['utterance']\n",
    "            if phrase=='': continue\n",
    "            if data[dindex]['dialogue'][i+1]['data'].get('slots'):\n",
    "                slot = data[dindex]['dialogue'][i+1]['data']['slots']\n",
    "                for k,v in slot.items():\n",
    "                    vv = get_phrase(phrase,v)\n",
    "                    slot[k]=vv\n",
    "                if slot.get('poi'): \n",
    "                    #task = task+'_request_poi'\n",
    "                    slot.pop('poi')\n",
    "                if slot.get('address'):\n",
    "                    #task = task+'_request_address'\n",
    "                    slot.pop('address')\n",
    "            else:\n",
    "                slot = {'dummy' : '<DUMMMMMMYYYYYY!!>'}\n",
    "            if data[dindex]['dialogue'][i+1]['data']['end_dialogue']:\n",
    "                task = 'thanks'\n",
    "            bio = make_iob(phrase,slot)\n",
    "            diag.append([nltk.word_tokenize(phrase),bio,task])\n",
    "        else:\n",
    "            phrase = data[dindex]['dialogue'][i]['data']['utterance']\n",
    "            diag.append([nltk.word_tokenize(phrase),'BOT','BOT'])\n",
    "    \n",
    "    last = data[dindex]['dialogue'][-1]['data']['utterance']\n",
    "    diag.append([nltk.word_tokenize(last),'BOT','BOT'])\n",
    "    diags[meta_task].append(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE = ['weather','navigate','schedule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for com in combinations_with_replacement(CASE,3):\n",
    "    case.append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_diags=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM):\n",
    "    c = random.choice(case)\n",
    "    diag=[]\n",
    "    for cc in c:\n",
    "        origin = random.choice(diags[cc])\n",
    "        diag.extend(origin)\n",
    "    recom_diags.append(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recom_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recom_diags,open('context_train.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.iob','w',encoding='utf-8') as f:\n",
    "    for diag in recom_diags:\n",
    "        for utter in diag:\n",
    "            if utter[1]=='BOT':\n",
    "                f.write(' '.join(utter[0])+'\\n')\n",
    "            else:\n",
    "                f.write(' '.join(utter[0])+\"|||\"+' '.join(utter[1])+\"|||\"+utter[2]+\"\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 단위 분류 및 레이블링\n",
    "* 오류 수정\n",
    "* 모델링 (우선 도메인부터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
